---
format: 
    revealjs:
        theme: night
        transition: fade
        css: css/styles.css
        footer: '<a href="https://github.com/mrguyperson/leibniz_interview" target="_blank">View on GitHub</a>'
title: "Data Analyst in Bioinformatics and Ecology"
author: Theodore Hermann
date: 21 August, 2025
title-slide-attributes:
  data-notes: |
    Hi everyone ‚Äî and thanks so much for the chance to talk with you today. I‚Äôm Ted Hermann, and I‚Äôd like to spend the next few minutes walking you through my background, how I see my experience aligning with this position, and why I‚Äôm genuinely excited about the opportunity to support your work at IGB.

---

## Background
:::: {.columns}
::: {.column width="75%"}

- Ph.D. Biology, M.P.A. ‚Äì bridging science, policy, and data
- 7+ years experience in data science, modeling, and analysis
- Open science advocate: reproducible, transparent workflows
- Consistently deliver tools, analyses, and support that help others succeed
:::

::: {.column width="25%"}
![](images/catfish.png){.absolute height="150" top="90" right="80"}
![](images/code.png){.absolute height="80" top="240" right="120"}
![](images/opensci.png){.absolute height="100" top="350" right="120"}
![](images/service.png){.absolute height="100" top="450" right="120"}


:::

::::

::: {.notes}

<!-- I come from a background that brings together freshwater ecology, Earth science, policy, and data science. This broad exposure to different areas of study has allowed me to succeed in projects with diverse knowledge bases. Over the years, I‚Äôve worked with ecological data, spatial data, simulation models, and statistical analysis. 

I have extensive experience addressing these problems with tools like R, Python, NetLogo, QGIS, git, and Docker in a way that supports open and reproducible science. -->
<!-- My career has combined freshwater biology, programming, and a focus on helping others succeed with data.

I‚Äôve worked in research, consulting, and freelance positions but the common thread has been developing tools, analyses, and workflows that other people ‚Äî scientists, engineers, decision makers ‚Äî can actually use.

I understand that this role is intended primarily as a service-oriented position, and that‚Äôs actually one of the reasons I was so strongly drawn to it when I first saw the posting. In my career so far, even though my title has often been ‚Äòresearch scientist‚Äô or ‚Äòpostdoctoral researcher,‚Äô a significant portion of my work has been in direct service to others‚Äô projects. -->

My background combines biology, programming, and applied problem solving. Across research, consulting, and freelance work, the common thread has been creating tools, workflows, and analyses that others ‚Äî scientists, engineers, decision makers ‚Äî can actually use. That service focus is one of the things that really drew me to this role. Let me give you a few concrete examples of how that‚Äôs looked in practice.

:::


## Experience {.smaller}

::: {.notes}
<!-- sDuring my postdoctoral work at UC Santa Cruz, I helped develop FHAST (Fish Habitat Assessment and Simulation Tool). My job wasn‚Äôt just to publish ‚Äî my team had been commissioned to deliver a working piece of software to our funders that could inform real-world river management decisions. I owned data acquisition and cleaning, wrote R sub-models for predator‚Äìprey, adult migration pathfinding, and habitat cover, and integrated NetLogo and QGIS behind a clean workflow. I packaged everything in targets and Docker so others could rerun scenarios without touching my machine. We iterated with stakeholders on timelines, shipped documentation and training, and the result was a reproducible tool managers could use directly for A/B habitat decisions. -->

During my postdoctoral work at UC Santa Cruz, I helped develop FHAST (Fish Habitat Assessment and Simulation Tool). My job wasn‚Äôt just to publish ‚Äî my team had been commissioned to deliver a working piece of software to our funders that could inform real-world river management decisions. That meant working with fixed timelines, gathering requirements from stakeholders, iterating on feedback, and ensuring the final product was usable and well-documented for others.

<!-- At UC Santa Cruz, I was part of a team that developed FHAST, a simulation toolkit used by river managers. That meant integrating field and environmental data, building tools in R and NetLogo, and working within a structured timeline ‚Äî not just writing a paper, but actually delivering software that could be used by non-technical stakeholders. -->

:::

üêü Postdoc: FHAST (UC Santa Cruz)

- Open-source agent-based modeling toolkit for fish habitat analysis ‚Üí Allowed research groups and agencies to run habitat simulations reproducibly and without advanced coding expertise.
<!-- - Built an agent-based modeling toolkit for salmon habitat management so river managers could directly test habitat restoration scenarios
- Integrated R, NetLogo, and GIS workflows into a cohesive product
- Project included timelines, deliverables, and stakeholder input to ensure the tool met real-world needs on schedule -->

. . . 

::: {.notes}
In a two-month engagement with the Electric Power Research Institute, I moved into a physics and electrical engineering domain to build a Python pipeline that predicts defects in electric equipment. My team's modeling pipeline cut error rates in half compared to the industry benchmark, but more importantly for our client, we delivered a maintainable repo, a white paper, and a retraining script so their engineers could update the model without us.

<!-- I also consulted for the Electric Power Research Institue where I worked with engineers and physicists to build a reusable predictive machine learning pipeline in Python while communicating frequently with our stakeholder. Here again, the outcome wasn‚Äôt about my name on a product ‚Äî but to provide something useful, understandable, and dependable. -->

 <!-- To enable a smooth project handover and facilitate long-term use, my team and I carefully documented the pipeline and workflow and wrote a detailed white paper. Again, the focus was on enabling long-term stakeholder success. -->

:::


üîå Consultation: EPRI Heat Exchanger ML Pipeline

- Machine learning pipeline for heat exchanger fault detection ‚Üí Helped engineers rapidly identify component failures, reducing downtime and maintenance costs.

<!-- - Used machine learning to help electrical engineers predict sensitive component failures earlier
- Built and optimized a Python ML pipeline to improve fault detection and reduce unplanned downtime
- Delivered results in an unfamiliar domain demonstrating adaptability and helping the client solve a persistent operational problem quickly -->

. . .

::: {.notes}

<!-- Finally, as a freelance editor, I edited hundreds of manuscripts and a couple dozen books to help researchers and companies clarify and polish their work ‚Äî whether that helping non-native English speakers prepare their work for English-language journal, respond to editors, or simply find grammatical errors in long texts. I had to meet tight deadlines, adapt to a wide range of subject matter, and strictly adhere to a variety of style guidelines. This was also a pure service role in which I adapted my approach to match clients' style and goals and where my success was measured by client success. -->

Alongside research, I spent years as a technical/copy editor for scientists and companies. My job was to make other people‚Äôs work land: structure arguments, clarify jargon, meet style guides, and hit deadlines. I worked extensively with non-native English researchers to ensure that language wasn't a barrier to their work reaching a large audience. This role taught me to listen, adapt to domain context quickly, and keep the spotlight on the client ‚Äî pure service work where success is measured by their success.

<!-- Finally, alongside research, I‚Äôve also worked for years as a technical and copy editor.
This was pure service work ‚Äî my role was to sharpen other people‚Äôs writing so they could succeed with journals, funders, and clients.
It trained me to listen closely, deliver on time, and keep the spotlight on others. -->

:::


üìù Freelance: Technical Editing & Writing Support

<!-- - Edited technical manuscripts, reports, and proposals ‚Üí Enabled authors and organizations to communicate findings more clearly and meet publication standards. -->
<!-- - Edited manuscripts, reports, and books so clients could meet publishing standards
- Followed strict deadlines and adherence to a number of style guides
- Success measured purely by client success -->
- Editor for scholars & companies ‚Üí helped clients publish, secure grants, and communicate clearly

<!-- - Focused on clarity, consistency, and service to authors‚Äô goals -->

## Reproducible Science {.smaller}
::: {.notes}

I‚Äôd also like to say a few words about my open science workflow. First, I haven't been following these practices simply because they were required at a previous job but rather due to frequent problems and frustrations that my team and I ran into:

Sharing work lead to version control problems (data_FINAL_copy2.xlsx).

Colleagues (including me) making mistakes that were hard to trace.

Code that ran on one person‚Äôs computer but not another's.

Analyses buried in giant scripts that were impossible to debug six months later.

And too often, no clear link between raw data and published results.

Each time, I had to find solutions. Over time, I built a toolkit and workflow that‚Äôs now part of my professional DNA ‚Äî reproducible pipelines, controlled environments, version control, and documentation so that results can actually be trusted, reused, and built on.
:::

Frequent problems:

. . .

- Sharing, aka `data_FINAL_copy2.xlsx`
- Hidden human errors
- Well, it worked on my computer ü§∑‚Äç‚ôÄÔ∏è
- *6 months later*: ERROR in line 1,504\...
- Raw data ‚Üí manuscript (mystery steps in between)
- so many more\...

## Reproducible Science {.smaller}

Solutions:

:::: {.columns}

::: {.column width="75%"}


- Pipelines so analyses can be rerun, verified, and adapted without guesswork


:::

::: {.column width="25%"}

![](images/targets.png){.absolute height="70" right="140" top="140"}

:::

::::

::: {.fragment .fade-in-then-out .overlay}

```{.r}
library(targets)
library(tarchetypes)
library(here)

tar_option_set(
  packages = c("tidyverse")
)

tar_source()

list(
    tar_target(
        name = path,
        command = here("path/to/data.csv"),
        format = "file"
    ),
    tar_target(
        name = raw_data,
        command = read_data(path)
    ),
    tar_target(
        name = analyzed_data,
        command = analyze_data(raw_data)
    ),
    tar_render(
        name = manuscript,
        path = here("path/to/manuscript.qmd")
    )
)
```
:::


::: {.notes}

<!-- Organizing projects with simple file structures and the use of a pipeline manager. -->

I organize projects with simple file structures and a pipeline manager, so anyone can rerun analyses easily and pick up my work without confusion.

**For code block:**

My current go-to pipeline tool in R is the targets package. (Here is code for a simplified example). Targets allows for a whole project to be controlled using a single command, from reading in raw data to producing a manuscript, leaving no ambiguity how you did something.
These tools elimanate the thousand-line do-it-all scripts that work for one project but can't easily transfer to a new one and are challenging to debug. This reduces human error and makes the flow of the project clear.

:::

## Reproducible Science {.smaller}
Solutions:

:::: {.columns}

::: {.column width="75%"}


- Pipelines so analyses can be rerun, verified, and adapted without guesswork


- Control the environment, e.g., using Docker and renv, to prevent the "well, it worked on my computer" problem

:::

::: {.column width="25%"}

![](images/targets.png){.absolute height="70" right="140" top="140"}

![](images/docker.png){.absolute height="70" right="160" top="220"}
![](images/renv.png){.absolute height="70" right="60" top="220"}

:::

::::


::: {.notes}

<!-- Next is making sure that anyone can run your code exactly the way you did. That's where Docker comes in to create a consistent computing environment regardless of the machine you're using; using something like renv ensures that package versions are the same as those used in development so nothing breaks. -->

I use Docker and renv to lock environments and package versions. That means collaborators and future users can run my code exactly as I did, without breakages or setup headaches.

:::



## Reproducible Science {.smaller}
Solutions:

:::: {.columns}

::: {.column width="75%"}


- Pipelines so analyses can be rerun, verified, and adapted without guesswork


- Control the environment, e.g., using Docker and renv, to prevent the "well, it worked on my computer" problem

- Version control (e.g., git and GitHub) to allow others inside and outside the project to contribute and reuse code


:::

::: {.column width="25%"}

![](images/targets.png){.absolute height="70" right="140" top="140"}

![](images/docker.png){.absolute height="70" right="160" top="220"}
![](images/renv.png){.absolute height="60" right="60" top="220"}

![](images/git.png){.absolute height="70" right="140" top="310"}


:::

::::


::: {.notes}

Hosting projects on GitHub makes the entire workflow ‚Äî data, code, and results ‚Äî transparent and accessible, so teammates can track changes, contribute, or reuse pieces of the pipeline.

:::

## Reproducible Science {.smaller}
Solutions:

:::: {.columns}

::: {.column width="75%"}


- Pipelines so analyses can be rerun, verified, and adapted without guesswork

- Control the environment, e.g., using Docker and renv, to prevent the "well, it worked on my computer" problem

- Version control (e.g., git and GitHub) to allow others inside and outside the project to contribute and reuse code

- Documentation so future researchers could find and understand the work without my direct involvement

:::

::: {.column width="25%"}

![](images/targets.png){.absolute height="70" right="140" top="140"}

![](images/docker.png){.absolute height="70" right="160" top="220"}
![](images/renv.png){.absolute height="70" right="60" top="220"}

![](images/git.png){.absolute height="60" right="140" top="310"}

![](images/quarto.png){.absolute height="40" right="90" top="410"}

:::

::: {.notes}

Documenting pipelines clearly, so years later others can still rerun, adapt, or extend my work ‚Äî turning one person‚Äôs analysis into a lasting resource. And new tools like Quarto make it easy to integrate documents directly into the pipeline itself.

:::

::::


## Conclusion {.smaller}

What would I bring to IGB and the Research Data Management group?



. . .

- Broad skillset plus freshwater expertise so I can join new projects quickly and contribute value early

::: {.notes}

I have the experience to join new projects and identify what I need to learn to become productive and helpful right away. My knowledge of freshwater biology should make that process even faster at IGB.

:::

. . .

- Dedication to service-oriented roles both inside and outside knowledge base

::: {.notes}

I have a track record of success in service roles across a number of domains, and I'd be excited to continue that at IGB. 

:::

. . .

- Extensive coding, data, and software development skills

::: {.notes}

My coding and analytics toolkit is pretty diverse, and I am constantly adding to it whenever I can, even in my free time.

:::

. . .

- Commitment to open, reproducible methods so colleagues can trust, share, and build on our results

::: {.notes}

I love data and analytics and creating the digital infrastructure to make it useful to others and easy to share and audit.

:::
. . .

::: {.notes}

In short: I have the technical range to work on many types of projects, the domain knowledge to be successful in freshwater ecology, a passion for open science, and the mindset to keep the focus on service. When I saw this job posting, I thought ‚Äî this is it, this is the job. It‚Äôs a chance to bring together my passion for analytics, freshwater biology, and building software tools to help research projects succeed, where success is measured not just by my own outputs, but by how much I can help the team and stakeholders succeed with their goals. 

:::


## Questions?

::: {.notes}

Just a final note: this talk was made using Quarto and all files are available on Github!

:::
