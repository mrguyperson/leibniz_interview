---
format: 
    revealjs:
        theme: moon
        transition: fade
        css: css/styles.css
        footer: '<a href="https://github.com/mrguyperson/leibniz_interview" target="_blank">View on GitHub</a>'
title: "Data Analyst in Bioinformatics and Ecology"
author: Theodore Hermann
date: 21 August, 2025
title-slide-attributes:
  data-notes: |
    Hi everyone ‚Äî and thanks so much for the chance to talk with you today. I‚Äôm Ted Hermann, and I‚Äôd like to spend the next few minutes walking you through my background, how I see my experience aligning with this position, and why I‚Äôm genuinely excited about the opportunity to support your work at IGB.

---

## Background
- Degrees in Earth science, public administration, and biology
- Experienced with a variety of data analytics tools
- Track record of Open Science practices
- Several years of experience in client-facing roles

::: {.notes}

I come from a background that brings together freshwater ecology, Earth science, policy, and data science. This broad exposure to different areas of study has allowed me to succeed in projects with diverse knowledge bases. Over the years, I‚Äôve worked with ecological data, spatial data, simulation models, and statistical analysis. 

I have extensive experience addressing these problems with tools like R, Python, NetLogo, QGIS, git, and Docker in a way that supports open and reproducible science.

And although my resume highlights my background in academia, most of the roles I've had over the last several years or so have been collaborative and service-focused.


:::


## Experience {.smaller}


::: {.notes}

At UC Santa Cruz, I was part of a team that developed FHAST, a simulation toolkit used by river managers. That meant integrating field and environmental data, building tools in R and NetLogo, and working within a structured timeline ‚Äî not just writing a paper, but actually delivering software that could be used by non-technical stakeholders.

:::

üß∞ Postdoc: FHAST (UC Santa Cruz)

- Built an agent-based modeling toolkit for salmon habitat management so river managers could directly test habitat restoration scenarios
- Integrated R, NetLogo, and GIS workflows into a cohesive product
- Project included timelines, deliverables, and stakeholder input to ensure the tool met real-world needs on schedule

. . . 

::: {.notes}

I also performed a two month consultation with the Electric Power Research Institue. Here, I had to pivot into a new domain ‚Äî working with engineers and physicists to build a predictive machine learning pipeline in Python while communicating frequently with our stakeholder. That experience really strengthened my ability to pick up new subject matter quickly.

:::


üß™ EPRI Consulting

- Used machine learning to help electrical engineers predict sensitive component failures earlier
- Built and optimized a Python ML pipeline to improve fault detection and reduce unplanned downtime
- Delivered results in an unfamiliar domain demonstrating adaptability and helping the client solve a persistent operational problem quickly

. . .

::: {.notes}

Finally, as a freelance editor, I spent years helping researchers and companies clarify and polish their work ‚Äî whether that was writing original manuscripts, responding to editors, writing books, or simply finding grammatical errors. This was a pure service role, where success was measured by my clients' success.

:::


üìù Technical Editing & Writing Support

- Edited manuscripts, reports, and books so clients could meet publishing standards
- Followed strict deadlines and adherence to a number of style guides
- Success measured purely by client success

## My Approach to Reproducible Science {.smaller}

::: {.notes}

Now I'll spend just a few moments outlining some of the steps I find most important for running open, reproducible, and shareable projects.

**For code block:**

My current go-to pipeline tool in R is the targets package. (Here is code for a simplified example). Targets allows for a whole project, from reading in data to producing a manuscript, to be controlled using a single command. It has an opinionated structure that forces each step of a project to be a single output produced by a function and upstream inputs. Built in hasing means that only steps with updated inputs get run again when rereunning the pipeline, thus saving time. Pipelines like this enforce an organized project structure, reduce human error, and make the flow of the project clear.

:::

. . .

- Communication: what are the inputs and desired outputs of a project?

::: {.notes}

First (step one or even step zero), it's critical to find out what the project needs are, especially the data and the expectations for the end of the project. Clearly defining end points is crucial to not waste time on the hard work that goes on between.

:::

. . .

- Develop a pipeline to keep things organized so colleagues can rerun, verify, and adapt results without guesswork

::: {.notes}

A simple file structure and the use of a pipeline manager keep a project organized, simple to run, and easy to maintain.

:::

::: {.fragment .fade-in-then-out .overlay}

```{.r}
library(targets)
library(tarchetypes)
library(here)

tar_option_set(
  packages = c("tidyverse")
)

tar_source()

list(
    tar_target(
        name = path,
        command = here("path/to/data.csv"),
        format = "file"
    ),
    tar_target(
        name = raw_data,
        command = read_data(path)
    ),
    tar_target(
        name = analyzed_data,
        command = analyze_data(raw_data)
    ),
    tar_render(
        name = manuscript,
        path = here("path/to/manuscript.qmd")
    )
)
```
:::

. . .

- Control the environment, e.g., using Docker and renv, to prevent the "well, it worked on my computer" problem

::: {.notes}

For others to easily see and repeat what you did, it's crucial they can run the analysis using the same environment you did. That's where Docker comes in to create a consistent computing environment regardless of the computer you're using, and using something like renv ensures that package versions are the same as those used in development.

:::
 
. . .

- Use version control (e.g., git and GitHub) to allow others inside and outside the project to contribute and reuse code

::: {.notes}

Finally, hosting everything on platforms like github simplifies collaboration, transparency and availability of your entire project, not just the data set you used.

:::

. . .

- Documentation so future researchers could find and understand the work without my direct involvement

::: {.notes}

Leaving behind clear documentation allows others in the future to make use of all this work even if years have gone by.

:::

## Conclusion {.smaller}

What would I bring to IGB and the Research Data Management group?



. . .

- Broad skillset plus freshwater expertise so I can join new projects quickly and contribute value early

::: {.notes}

My diverse education and work experience have helped me understand how to join a new project and identify what I need to learn to become productive and helpful right away. My knowledge of freshwater biology should make that process even faster.

:::

. . .

- Dedication to service-oriented roles both inside and outside knowledge base

::: {.notes}

I have a track record of success in service roles across a number of domains, and I'd be excited to continue that experience at IGB. 

:::

. . .

- Extensive coding, data, and software development skills

::: {.notes}

My coding and analytics toolkit is pretty diverse, and I am constantly adding to it whenever I can, even in my free time.

:::

. . .

- Commitment to open, reproducible methods so colleagues can trust, share, and build on our results

::: {.notes}

I love data and analytics and creating the digital infrastructure to make it useful to others and easy to share and audit.

:::


::: {.notes}

In short: I have the technical range to work on many types of projects, the domain knowledge to be relevant in freshwater ecology, a passion for open science, and the mindset to keep the focus on service.

:::

## Conclusion
Just some ideas...

::: {.notes}

These are just a couple things I thought could be cool to explore if I have the opportunity to work with you all.

:::

. . .

- Run coding and data workshops


::: {.notes}

In my previous role, I did a few small live coding demos and workshops for my team to showcase some packages, tech, and practices that I found really helpful in my work and thought others might be interested in as well. These were a lot of fun and well received, and I think it might be cool to do something like this again, maybe in a more formal way.

:::

. . .

- Publish to CRAN, rOpenSci, Bioconductor, etc.

::: {.notes}

In the spirit of sharing and open research, I'd like to eventually get some helpful packages up to CRAN depending on the work we'd end up doing.

:::

## Questions?

::: {.notes}

Just a final note: this talk was made using Quarto and all files are available on Github!

:::