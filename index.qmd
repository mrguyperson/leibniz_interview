---
format: 
    revealjs:
        theme: moon
        transition: fade
        css: css/styles.css
        footer: '<a href="https://github.com/mrguyperson/leibniz_interview" target="_blank">View on GitHub</a>'
title: "Data Analyst in Bioinformatics and Ecology"
author: Theodore Hermann
date: 21 August, 2025
title-slide-attributes:
  data-notes: |
    Hi everyone ‚Äî and thanks so much for the chance to talk with you today. I‚Äôm Ted Hermann, and I‚Äôd like to spend the next few minutes walking you through my background, how I see my experience aligning with this position, and why I‚Äôm genuinely excited about the opportunity to support your work at IGB.

---

## Background
- Ph.D. Biology, M.P.A. ‚Äì bridging science and applied problem solving
- 7+ years experience in data science, modeling, and statistical analysis
- Passion for reproducible workflows & open science
- Proven track record in collaborative, service-driven projects

::: {.notes}

<!-- I come from a background that brings together freshwater ecology, Earth science, policy, and data science. This broad exposure to different areas of study has allowed me to succeed in projects with diverse knowledge bases. Over the years, I‚Äôve worked with ecological data, spatial data, simulation models, and statistical analysis. 

I have extensive experience addressing these problems with tools like R, Python, NetLogo, QGIS, git, and Docker in a way that supports open and reproducible science. -->
My career has combined freshwater biology, programming, and a focus on helping others succeed with data.

I‚Äôve worked in research, consulting, and freelance positions but the common thread has been developing tools, analyses, and workflows that other people ‚Äî scientists, engineers, decision makers ‚Äî can actually use.

I understand that this role is intendedd primarily as a service-oriented position, and that‚Äôs actually one of the reasons I was so strongly drawn to it when I first saw the posting. In my career so far, even though my title has often been ‚Äòresearch scientist‚Äô or ‚Äòpostdoctoral researcher,‚Äô a significant portion of my work has been in direct service to others‚Äô projects.



:::


## Experience {.smaller}

. . .

::: {.notes}

For example, during my postdoctoral work at UC Santa Cruz, I helped develop FHAST (Fish Habitat Assessment and Simulation Tool), which was built using R and Netlogo. My job wasn‚Äôt just to publish ‚Äî it was to deliver a working piece of software to our funders that could actually inform real-world river management decisions. That meant working with fixed timelines, gathering requirements from stakeholders, iterating on feedback, and ensuring the final product was usable and well-documented for others.

<!-- At UC Santa Cruz, I was part of a team that developed FHAST, a simulation toolkit used by river managers. That meant integrating field and environmental data, building tools in R and NetLogo, and working within a structured timeline ‚Äî not just writing a paper, but actually delivering software that could be used by non-technical stakeholders. -->

:::

üêü Postdoc: FHAST (UC Santa Cruz)

- Built an open-source agent-based modeling toolkit for fish habitat analysis ‚Üí Allowed research groups and agencies to run habitat simulations reproducibly and without advanced coding expertise.
<!-- - Built an agent-based modeling toolkit for salmon habitat management so river managers could directly test habitat restoration scenarios
- Integrated R, NetLogo, and GIS workflows into a cohesive product
- Project included timelines, deliverables, and stakeholder input to ensure the tool met real-world needs on schedule -->

. . . 

::: {.notes}

I also performed a two month consultation with the Electric Power Research Institue. Here, I worked with engineers and physicists to build a predictive machine learning pipeline in Python while communicating frequently with our stakeholder. To enabled a smooth project handover and facilitate long-term use, my team and I carefully documented the pipeline and workflow, and optimized retraining processes to reduce time needed for model updates. Again, the focus was on enabling stakeholder success.

:::


üîå Consultation: EPRI Heat Exchanger ML Pipeline

- Improved machine learning pipeline for heat exchanger fault detection ‚Üí Helped engineers rapidly identify component failures, reducing downtime and maintenance costs.

<!-- - Used machine learning to help electrical engineers predict sensitive component failures earlier
- Built and optimized a Python ML pipeline to improve fault detection and reduce unplanned downtime
- Delivered results in an unfamiliar domain demonstrating adaptability and helping the client solve a persistent operational problem quickly -->

. . .

::: {.notes}

Finally, as a freelance editor, I spent years helping researchers and companies clarify and polish their work ‚Äî whether that was writing original manuscripts, responding to editors, writing books, or simply finding grammatical errors in long texts. I had to meet tight deadlines, adapt to a wide range of subject matter, and strictly adhere to a variety of style guidelines. This was also a pure service role in which I adapted my approach to match clients' style and goals and where success was measured by their success.

:::


üìù Freelance: Technical Editing & Writing Support

- Edited technical manuscripts, reports, and proposals ‚Üí Enabled authors and organizations to communicate findings more clearly and meet publication standards.
<!-- - Edited manuscripts, reports, and books so clients could meet publishing standards
- Followed strict deadlines and adherence to a number of style guides
- Success measured purely by client success -->

## My Approach to Reproducible Science {.smaller}

::: {.notes}

Now I'll spend just a few moments outlining some of the steps I find most important to me when running open, reproducible, and shareable projects.

**For code block:**

My current go-to pipeline tool in R is the targets package. (Here is code for a simplified example). Targets allows for a whole project, from reading in raw data to producing a manuscript, to be controlled using a single command.<!-- It has an opinionated structure that forces each step of a project to be a single output produced by a function and upstream inputs. Built in hasing means that only steps with updated inputs get run again when rereunning the pipeline, thus saving time.  -->
These tools enforce an organized code structure -- no more thousand-line do-it-all scripts -- reduce human error, and make the flow of the project clear.

:::

. . .

- Communication: what are the inputs and desired outputs of a project?

::: {.notes}

(step one or even step zero: Communication) It's critical to find out what the project needs are, especially the data and the expectations for project outcomes. Clearly defining end points is crucial to not waste time on the all the hard work that goes on between.

:::

. . .

- Developing a pipeline to keep things organized so colleagues can rerun, verify, and adapt results without guesswork

::: {.notes}

Developing a pipeline: A simple file structure and the use of a pipeline manager keep a project organized, straighforward to run, and easy to maintain.

:::

::: {.fragment .fade-in-then-out .overlay}

```{.r}
library(targets)
library(tarchetypes)
library(here)

tar_option_set(
  packages = c("tidyverse")
)

tar_source()

list(
    tar_target(
        name = path,
        command = here("path/to/data.csv"),
        format = "file"
    ),
    tar_target(
        name = raw_data,
        command = read_data(path)
    ),
    tar_target(
        name = analyzed_data,
        command = analyze_data(raw_data)
    ),
    tar_render(
        name = manuscript,
        path = here("path/to/manuscript.qmd")
    )
)
```
:::

. . .

- Control the environment, e.g., using Docker and renv, to prevent the "well, it worked on my computer" problem

::: {.notes}

For others to easily see and repeat what you did, it's crucial they can run the analysis using the same environment you did. That's where Docker comes in to create a consistent computing environment regardless of the computer you're using, and using something like renv ensures that package versions are the same as those used in development.

:::
 
. . .

- Use version control (e.g., git and GitHub) to allow others inside and outside the project to contribute and reuse code

::: {.notes}

Hosting everything on platforms like github simplifies collaboration, transparency and availability of your entire project, not just the data set you used.

:::

. . .

- Documentation so future researchers could find and understand the work without my direct involvement

::: {.notes}

Finally, leaving behind clear documentation allows others in the future to make use of all this work even if years have gone by.

:::

## Conclusion {.smaller}

What would I bring to IGB and the Research Data Management group?



. . .

- Broad skillset plus freshwater expertise so I can join new projects quickly and contribute value early

::: {.notes}

My diverse education and work experience have helped me understand how to join a new project and identify what I need to learn to become productive and helpful right away. My knowledge of freshwater biology should make that process even faster.

:::

. . .

- Dedication to service-oriented roles both inside and outside knowledge base

::: {.notes}

I have a track record of success in service roles across a number of domains, and I'd be excited to continue that experience at IGB. 

:::

. . .

- Extensive coding, data, and software development skills

::: {.notes}

My coding and analytics toolkit is pretty diverse, and I am constantly adding to it whenever I can, even in my free time.

:::

. . .

- Commitment to open, reproducible methods so colleagues can trust, share, and build on our results

::: {.notes}

I love data and analytics and creating the digital infrastructure to make it useful to others and easy to share and audit.

:::
. . .

::: {.notes}

In short: I have the technical range to work on many types of projects, the domain knowledge to be relevant in freshwater ecology, a passion for open science, and the mindset to keep the focus on service. When I saw this job posting, I thought ‚Äî this is it, this is the job. It‚Äôs a chance to bring together my analytical skills and experience in freshwater biology to help research projects succeed, where success is measured not just by my own outputs, but by how much I can help the team and stakeholders succeed with their goals. 

:::

## Conclusion
Just some ideas...

::: {.notes}

These are just a couple things I thought could be cool to explore if I have the opportunity to work with you all.

:::

. . .

- Run coding and data workshops


::: {.notes}

In my previous role, I did a few small live coding demos and workshops for my team to showcase some packages, tech, and practices that I found really helpful in my work and thought others might be interested in as well. These were a lot of fun and well received, and I think it might be cool to do something like this again, maybe in a more formal way.

:::

. . .

- Publish to CRAN, rOpenSci, Bioconductor, etc.

::: {.notes}

In the spirit of sharing and open research, I'd like to eventually get some helpful packages up to CRAN depending on the work we'd end up doing.

:::

## Questions?

::: {.notes}

Just a final note: this talk was made using Quarto and all files are available on Github!

:::