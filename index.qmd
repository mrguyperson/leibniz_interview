---
format: 
    revealjs:
        theme: moon
        transition: fade
        css: css/styles.css
        footer: '<a href="https://github.com/mrguyperson/leibniz_interview" target="_blank">View on GitHub</a>'
title: "Data Analyst in Bioinformatics and Ecology"
author: Theodore Hermann
date: 21 August, 2025
title-slide-attributes:
  data-notes: |
    Hi everyone ‚Äî and thanks so much for the chance to talk with you today. I‚Äôm Ted Hermann, and I‚Äôd like to spend the next few minutes walking you through my background, how I see my experience aligning with this position, and why I‚Äôm genuinely excited about the opportunity to support your work at IGB.

---

## Background
- Degrees in Earth science, public administration, and biology
- Experienced with a variety of data analytics tools
- Track record of Open Science practices
- Several years of experience in client-facing roles

::: {.notes}

I come from a background that brings together freshwater ecology, Earth science, policy, and data science. This broad exposure to different areas of study has allowed me to succeed in projects with diverse knowledge bases. Over the years, I‚Äôve worked with ecological data, spatial data, simulation models, and statistical analysis. 

I have extensive experience solving these problems with tools like R, Python, NetLogo, QGIS, git, and Docker in a way that supports open and reproducible science.

And although my resume highlights my background in academia, most of the roles I've had over the several years or so have been collaborative and service-focused, which I will go into in just a moment.


:::


## Experience {.smaller}


::: {.notes}
At UC Santa Cruz, I was part of a team that developed FHAST, a simulation toolkit used by river managers. That meant integrating field and environmental data, building tools in R and NetLogo, and working within a structured timeline ‚Äî not just writing a paper, but actually delivering software that could be used by non-technical stakeholders.
:::

üß∞ FHAST (UC Santa Cruz)

- Commissioned modeling tool for endangered species
- Integrated R, NetLogo, and geospatial data in QGIS
- Project included timelines, deliverables, and stakeholder input

. . . 

::: {.notes}
I also performed a two month consultation with the Electric Power Research Institue. Here, I had to pivot into a new domain ‚Äî working with engineers and physicists to build a predictive machine learning pipeline in Python while communicating frequently wiht our stakeholder. That experience really strengthened my ability to pick up new subject matter quickly.
:::

üß™ EPRI Consulting

- Python ML pipeline for predictive maintenance
- Interdisciplinary: worked outside my field and delivered results fast

. . .

::: {.notes}
Finally, as a freelance editor, I spent years helping researchers and companies clarify and polish their work ‚Äî whether that was writing original manuscripts, responding to editors, writing books, or simply finding grammatical errors. The role was all about service and communication.
:::


üìù Technical Editing & Writing Support

- Helped researchers and clients publish, document, and organize work
- Fluent in communicating complex ideas clearly

## My Approach to Reproducible Science {.smaller}

::: {.notes}

Now I'll spend just a few moments outlining some of the steps I find most important for running open, reproducible, and shareable projects.

**For code block:**

My current go-to pipeline tool in R is the targets package. It allows for a whole project, from reading in data to producing a manuscript, to be controlled using a single command. Its opinionated structure forces each step of a project to be a single output produced by a function and upstream inputs. Steps are hashed so rerunning the pipeline will only run steps that were changed since the previous run. Pipelines like this enforce an organized project structure, reduce human error, and make the flow of the project clear.

:::

. . .

- Communication: what are the inputs and desired outputs of a project?

::: {.notes}

First, it's critical to find out what the project needs are, especially the data and what the expectations for the end of the project are. Clearly defining end points is crucial to not waste time on the hard work that goes on between.

:::

. . .

- Develop a pipeline to keep things organized

::: {.notes}

A simple file structure and the use of a pipeline manager keep your project organized, simple to run, and easy to maintain.

:::

::: {.fragment .fade-in-then-out .overlay}

```{.r}
library(targets)
library(tarchetypes)
library(here)

tar_option_set(
  packages = c("tidyverse")
)

tar_source()

list(
    tar_target(
        name = path,
        command = here("path/to/data.csv"),
        format = "file"
    ),
    tar_target(
        name = raw_data,
        command = read_data(path)
    ),
    tar_target(
        name = analyzed_data,
        command = analyze_data(raw_data)
    ),
    tar_render(
        name = manuscript,
        path = here("path/to/manuscript.qmd")
    )
)
```
:::

. . .

- Control the environment, e.g., using Docker and renv

::: {.notes}

For others to easily see and repeat what you did, it's crucial they can run the analysis using the same environment you did. That's where Docker comes in to create a consistent computing environment regardless of the computer you're using, and using something like renv ensures that package versions are the same as those used in development.

:::
 
. . .

- Keep it open (e.g., using git and GitHub) and iterate

::: {.notes}

Finally, hosting everything on platforms like github simplifies collaboration, transparency and availability of your entire project, not just the data set you used.

:::

## Conclusion

What would I bring to IGB and the Research Data Management group?



. . .

- Jack of all trades with freshwater expertise

::: {.notes}

My diverse education and work experience have helped me understand how to join a new project and identify what I need to learn to become productive and helpful right away. My knowledge of freshwater biology should make that process even faster.

:::

. . .

- Success in service-oriented roles both inside and outside knowledge base

::: {.notes}

I have a track record of success in service roles across a number of domains, and I'd be excited to bring that experience to IGB. 

:::

. . .

- Extensive coding, data, and software development skills

::: {.notes}

My coding and analytics toolkit is pretty diverse, and I am constantly adding to it whenever I can, even in my free time.

:::

. . .

- Passion for data analysis and open, reproducible methods

::: {.notes}

I love data and analytics and creating digital infrastructure that makes data useful to others and easy to share and audit.

:::

. . .

::: {.notes}

Overall, I'm very excited about this position because of how well it lines up with my interests and strengths: analyzing ecological datasets, supporting reproducible science, and collaborating across research teams. The focus on open, reproducible science and cross-disciplinary teamwork really aligns with the way I already approach my work.

:::

## Conclusion
Just some ideas...

::: {.notes}

These are just a couple things I thought could be cool to explore if I have the opportunity to work with you all.

:::

. . .

- Run coding and data workshops


::: {.notes}

In my previous role, I did a few small live coding demos and workshops for my team to showcase some packages, tech, and practices that I found really helpful in my work and thought others might be interested in as well. These were a lot of fun and well received, and I think it might be cool to do something like this again, maybe in a more formal way.

:::

. . .

- Publish to CRAN, rOpenSci, Bioconductor, etc.

::: {.notes}

In the spirit of sharing and open research, I'd like to eventually get some helpful packages up to CRAN depending on the work we'd end up doing.

:::

## Questions?

::: {.notes}

Just a final note: this talk was made using Quarto and all files are available on Github!

:::